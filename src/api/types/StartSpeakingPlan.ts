/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Vapi from "../index";

export interface StartSpeakingPlan {
    /**
     * This is how long assistant waits before speaking. Defaults to 0.4.
     *
     * This is the minimum it will wait but if there is latency is the pipeline, this minimum will be exceeded. This is really a stopgap in case the pipeline is moving too fast.
     *
     * Example:
     *
     * - If model generates tokens and voice generates bytes within 100ms, the pipeline still waits 300ms before outputting speech.
     *
     * Usage:
     *
     * - If the customer is taking long pauses, set this to a higher value.
     * - If the assistant is accidentally jumping in too much, set this to a higher value.
     *
     * @default 0.4
     */
    waitSeconds?: number;
    /**
     * This determines if a customer speech is considered done (endpointing) using the VAP model on customer's speech. This is good for middle-of-thought detection.
     *
     * Once an endpoint is triggered, the request is sent to `assistant.model`.
     *
     * Default `false` since experimental.
     *
     * @default false
     */
    smartEndpointingEnabled?: boolean;
    /**
     * This determines how a customer speech is considered done (endpointing) using the transcription of customer's speech.
     *
     * Once an endpoint is triggered, the request is sent to `assistant.model`.
     */
    transcriptionEndpointingPlan?: Vapi.TranscriptionEndpointingPlan;
}
